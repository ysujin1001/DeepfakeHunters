import base64, io, os, datetime, json
from fpdf import FPDF
from PIL import Image
from langchain_openai import ChatOpenAI
from langchain_core.prompts import PromptTemplate
from dotenv import load_dotenv
load_dotenv() 


# ==========================================================
# ğŸ“ 1. ê²°ê³¼ ì €ì¥ ê²½ë¡œ ì„¤ì •
# ==========================================================
BASE_RESULT_DIR = os.path.join("data", "results")
IMAGE_DIR = os.path.join(BASE_RESULT_DIR, "images")
PDF_DIR = os.path.join(BASE_RESULT_DIR, "pdfs")
LOG_DIR = os.path.join(BASE_RESULT_DIR, "logs")

# í´ë” ìë™ ìƒì„±
for folder in [BASE_RESULT_DIR, IMAGE_DIR, PDF_DIR, LOG_DIR]:
    os.makedirs(folder, exist_ok=True)

# ==========================================================
# ğŸ§  2. ë³´ê³ ì„œ ìƒì„± í•¨ìˆ˜
# ==========================================================
def generate_heatmap_report(result_data):
    """
    Grad-CAM íˆíŠ¸ë§µ ë¶„ì„ ì¤‘ì‹¬ì˜ PDF ë³´ê³ ì„œ ìƒì„±
    result_data ì˜ˆì‹œ:
    {
        "result": "Fake",
        "fake_probability": 0.873,
        "gradcam": "<base64>",
        "model_type": "korean",
        "model_name": "MobileNetV3-Small"
    }
    """

    # ------------------------------------------------------
    # 1. Grad-CAM ì´ë¯¸ì§€ ì €ì¥
    # ------------------------------------------------------
    gradcam_b64 = result_data["gradcam"]
    gradcam_bytes = base64.b64decode(gradcam_b64)
    gradcam_img = Image.open(io.BytesIO(gradcam_bytes))

    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    image_filename = f"gradcam_{timestamp}.png"
    gradcam_path = os.path.join(IMAGE_DIR, image_filename)
    gradcam_img.save(gradcam_path)

    # ------------------------------------------------------
    # 2. LangChain LLM í”„ë¡¬í”„íŠ¸ êµ¬ì„± ë° ì‹¤í–‰
    # ------------------------------------------------------
    prompt = PromptTemplate(
        input_variables=["result", "prob", "type"],
        template=(
            "ë„ˆëŠ” ë”¥í˜ì´í¬ íƒì§€ ì „ë¬¸ê°€ì•¼. ì•„ë˜ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ Grad-CAM íˆíŠ¸ë§µì„ í•´ì„í•´.\n\n"
            "ëª¨ë¸ ìœ í˜•: {type}\n"
            "ì˜ˆì¸¡ ê²°ê³¼: {result}\n"
            "ë”¥í˜ì´í¬ í™•ë¥ : {prob:.2f}%\n\n"
            "ë¶‰ì€ìƒ‰ ì˜ì—­ì€ ëª¨ë¸ì´ ë”¥í˜ì´í¬ íŒë‹¨ì˜ ê·¼ê±°ë¡œ ë³¸ ë¶€ë¶„ì´ì•¼. "
            "ì´ ì‹œê° ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ëª¨ë¸ì´ ì–´ë–»ê²Œ íŒë‹¨í–ˆëŠ”ì§€, "
            "í•©ì„± í”ì Â·í”¼ë¶€ ì§ˆê°Â·ì¡°ëª… ì™œê³¡ ë“± ì‹œê°ì  ê·¼ê±°ë¥¼ ê¸°ìˆ ì ìœ¼ë¡œ ë¶„ì„í•´ì¤˜. "
            "ë˜í•œ ì¸ê°„ ì „ë¬¸ê°€ì˜ ê´€ì ì—ì„œ ì‹ ë¢°ë„ì™€ í•œê³„ì ë„ í•¨ê»˜ ì„¤ëª…í•´ì¤˜."
            "ì•„ìš¸ëŸ¬, ì–¸ê¸‰í•˜ì§€ ì•Šì€ ì‹¬ì¸µ ê²°ê³¼ê°€ ìˆìœ¼ë©´ í•¨ê»˜ ìƒì„¸íˆ ì„¤ëª…í•´ì¤˜."
        ),
    )

    llm = ChatOpenAI(model="gpt-4o-mini")

    analysis_text = llm.invoke(
        prompt.format(
            type="í•œêµ­ì¸ ì´ë¯¸ì§€ ë¶„ì„ ëª¨ë¸" if result_data["model_type"] == "korean" else "ì™¸êµ­ì¸ ì´ë¯¸ì§€ ë¶„ì„ ëª¨ë¸",
            result=result_data["result"],
            prob=result_data["fake_probability"] * 100,
        )
    ).content

    # ------------------------------------------------------
    # 3. PDF ë³´ê³ ì„œ ìƒì„±
    # ------------------------------------------------------
    pdf = FPDF()
    pdf.add_page()

    # ì œëª©
    pdf.add_font("malgun", "", r"C:\Windows\Fonts\malgun.ttf", uni=True)   # ì¼ë°˜
    pdf.add_font("malgun", "B", r"C:\Windows\Fonts\malgunbd.ttf", uni=True)   # êµµì€ì²´
    pdf.set_font("malgun", "B", size=16)
    pdf.cell(0, 10, "ë”¥í˜ì´í¬ íˆíŠ¸ë§µ ë¶„ì„ ë³´ê³ ì„œ", ln=True, align="C")

    # (1) ë¶„ì„ ê°œìš”
    pdf.set_font("malgun", "B", size=13)
    pdf.cell(0, 10, "1. ë¶„ì„ ê°œìš”", ln=True)
    pdf.set_font("malgun", size=11)

    model_name = result_data.get("model_name", "MobileNetV3-Small (PyTorch)")
    model_type = "í•œêµ­ì¸ ì „ìš© ëª¨ë¸" if result_data["model_type"] == "korean" else "ì™¸êµ­ì¸ ì „ìš© ëª¨ë¸"
    analyzed_at = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    pdf.multi_cell(
        0,
        8,
        f"- ëª¨ë¸ëª…: {model_name}\n"
        f"- ëª¨ë¸ ìœ í˜•: {model_type}\n"
        f"- ë¶„ì„ ì¼ì‹œ: {analyzed_at}\n"
        f"- ì˜ˆì¸¡ ê²°ê³¼: {result_data['result']}\n"
        f"- ë”¥í˜ì´í¬ í™•ë¥ : {(result_data['fake_probability'] * 100):.2f}%\n",
    )

    # (2) Grad-CAM ì‹œê°í™”
    pdf.ln(8)
    pdf.set_font("malgun", "B", 13)
    pdf.cell(0, 10, "2. Grad-CAM ì‹œê°í™”", ln=True)
    pdf.image(gradcam_path, x=25, y=pdf.get_y() + 5, w=160)
    pdf.ln(95)

    # (3) LangChain ê¸°ë°˜ AI í•´ì„
    pdf.set_font("malgun", "B", 13)
    pdf.cell(0, 10, "3ï¸. LangChain ê¸°ë°˜ AI í•´ì„", ln=True)
    pdf.set_font("malgun", size=11)
    pdf.multi_cell(0, 7, analysis_text)

    # (4) ê²°ë¡  ë° ê¶Œì¥ ì¡°ì¹˜
    pdf.ln(5)
    pdf.set_font("malgun", "B", 13)
    pdf.cell(0, 10, "4ï¸. ê²°ë¡  ë° ê¶Œì¥ ì¡°ì¹˜", ln=True)
    pdf.set_font("malgun", size=11)
    pdf.multi_cell(
        0,
        7,
        "ë³¸ ë¶„ì„ì€ Grad-CAM ì‹œê° ì£¼ëª©ë„ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ì§„í–‰ë˜ì—ˆìŠµë‹ˆë‹¤.\n"
        "AIì˜ ê²°ê³¼ëŠ” ì°¸ê³ ìš©ìœ¼ë¡œ ì‚¬ìš©í•´ì•¼ í•˜ë©°, ë²•ì  íŒë‹¨ì´ë‚˜ ê³µì‹ ì¦ê±°ë¡œ ì‚¬ìš©ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n"
        "ê²°ê³¼ì˜ ì‹ ë¢°ë„ë¥¼ ë†’ì´ê¸° ìœ„í•´ ë‹¤ì–‘í•œ ì´ë¯¸ì§€ ì†ŒìŠ¤ë¡œ êµì°¨ ê²€ì¦ì„ ê¶Œì¥í•©ë‹ˆë‹¤.",
    )

    # PDF ì €ì¥
    pdf_filename = f"heatmap_report_{timestamp}.pdf"
    pdf_path = os.path.join(PDF_DIR, pdf_filename)
    pdf.output(pdf_path)

    # ------------------------------------------------------
    # 4. ë¡œê·¸ JSON ìƒì„± (DB ì—°ë™ ëŒ€ë¹„)
    # ------------------------------------------------------
    log_data = {
        "created_at": analyzed_at,
        "model_name": model_name,
        "model_type": model_type,
        "result": result_data["result"],
        "fake_probability": result_data["fake_probability"],
        "gradcam_image": gradcam_path,
        "pdf_path": pdf_path,
    }

    log_filename = f"report_log_{timestamp}.json"
    with open(os.path.join(LOG_DIR, log_filename), "w", encoding="utf-8") as f:
        json.dump(log_data, f, indent=4, ensure_ascii=False)

    # ìµœì¢… PDF ê²½ë¡œ ë°˜í™˜
    return pdf_path